{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект классификация комментариев\n",
    "\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Магазину нужен инструмент, который будет искать токсичные комментарии, то есть классифицировать их как негативные и позитивные, и отправлять их на модерацию. \n",
    "\n",
    "**Задача**\n",
    "\n",
    "Обучить модель классифицировать комментарии на позитивные и негативные. Метрика модели F1 должна быть не меньше 0.75\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "В данных находятся:\n",
    "- *text* в нём содержит текст комментария\n",
    "- *toxic* — целевой признак, показывающий позитивный это или негативный комментарий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала нужно подготовить комментарии для классифицирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import re\n",
    "import nltk\n",
    "import warnings\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import notebook\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toxic = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "None\n",
      "0    143346\n",
      "1     16225\n",
      "Name: toxic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_toxic.info())\n",
    "print(df_toxic['toxic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_toxic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст комментариев выглядит адекватным. Позитивных комментариев почти в 10 раз больше негативных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_toxic['text'].values.astype('U')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    new_text = re.sub(r'[^a-z]',' ',text)\n",
    "    split_text = new_text.split()\n",
    "    return ' '.join(split_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведена очистка комментариев от небуквенных символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_data = []\n",
    "for i in range(len(corpus)): \n",
    "    clean_text = clear_text(corpus[i])\n",
    "    tokens = nltk.word_tokenize(clean_text)\n",
    "    stemmer = EnglishStemmer(ignore_stopwords=False)\n",
    "    lemm = ' '.join([stemmer.stem(token) for token in tokens])\n",
    "    lemm_data.append(lemm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст до лемматизации и очистки: d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)\n",
      "Текст после лемматизации и очистки: d aww he match this background colour i m seem stuck with thank talk januari utc\n"
     ]
    }
   ],
   "source": [
    "print('Текст до лемматизации и очистки:', corpus[1])\n",
    "print('Текст после лемматизации и очистки:', lemm_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведена лемматизация текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_toxic['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_data_train, lemm_data_test, target_train, target_test = train_test_split(\n",
    "    lemm_data, target, test_size = 0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    114670\n",
      "1     12986\n",
      "Name: toxic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(target_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer()\n",
    "tf_idf_tr = count_tf_idf.fit_transform(lemm_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизованы признаки с помощью TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_te = count_tf_idf.transform(lemm_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = tf_idf_tr\n",
    "features_test = tf_idf_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе подготовки текста комментариев к обучению модели, была проведена его предобработка.\n",
    "\n",
    "- Убраны лишние (небуквенные) символы, лемматизированы комментарии. \n",
    "- Данные разбиты на обучающую и тестовую выборки. \n",
    "- Признаки векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n",
    "gs = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid=grid,\n",
    "                  scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param: {'C': 10.0, 'penalty': 'l2'} f1_score: 0.7695464434580939\n"
     ]
    }
   ],
   "source": [
    "gs.fit(features_train,target_train)\n",
    "print('param:', gs.best_params_, 'f1_score:', gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = {'model':'LogisticRegression', 'f1_score': gs.best_score_}\n",
    "rows.append(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha':[1, 10, 200, 265, 270, 275, 500],\n",
    "               'penalty':[\"l1\",\"l2\"]}\n",
    "\n",
    "gs2 =  GridSearchCV(SGDClassifier(class_weight='balanced'), param_grid=params,\n",
    "                    scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param: {'alpha': 1, 'penalty': 'l2'} f1_score: 0.12746218646236362\n"
     ]
    }
   ],
   "source": [
    "gs2.fit(features_train, target_train)\n",
    "print('param:', gs2.best_params_, 'f1_score:', gs2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = {'model':'SGDClassifier', 'f1_score': gs2.best_score_}\n",
    "rows.append(sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_params = {'alpha':[1, 10, 200, 265, 270, 275, 500]}\n",
    "\n",
    "gs3 =  GridSearchCV(RidgeClassifier(class_weight='balanced'), param_grid=ridge_params, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param: {'alpha': 10} f1_score: 0.7062091191763115\n"
     ]
    }
   ],
   "source": [
    "gs3.fit(features_train, target_train)\n",
    "print('param:', gs3.best_params_, 'f1_score:', gs3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = {'model':'RidgeClassifier', 'f1_score': gs3.best_score_}\n",
    "rows.append(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model  f1_score\n",
      "0  LogisticRegression  0.769546\n",
      "1     RidgeClassifier  0.706209\n",
      "2       SGDClassifier  0.127462\n"
     ]
    }
   ],
   "source": [
    "table_f1 = pd.DataFrame(rows)\n",
    "print(table_f1.sort_values(by='f1_score', ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучено 3 линейных модели. Лучший результат выдала Логистияеская Регрессия. Она показала удовлетворительный результат выше необходимого показателя - 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "значение f1 на тестовой выборке: 0.7686230248306998\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight='balanced', C=10.0, penalty='l2')\n",
    "model.fit(features_train, target_train)\n",
    "predictions = model.predict(features_test)\n",
    "f1_tfidf_te = f1_score(target_test, predictions)\n",
    "print('значение f1 на тестовой выборке:', f1_tfidf_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ИТОГ**\n",
    "\n",
    "- Проведена предобработка данных. (Лемматизирован и векторизован текст)\n",
    "\n",
    "- Обучено 3 линейных модели, чтобы отобрать из них ту, которая покажет лучший результат f1_score\n",
    "\n",
    "- Модель линейная регрессия показала удовлетворительный результат на тестовой выборке, f1_score больше 0.75 (f1 = 0.769)\n",
    "\n",
    "Модель сможет классифицировть комментарии на негативный и позитивный"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
